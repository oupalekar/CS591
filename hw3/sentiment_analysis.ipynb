{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import GloVe\n",
    "from attack import SubstitutionAttack\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imdb import IMDBDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>' '<unk>' 'the' ',' '.' 'of' 'to' 'and' 'in' 'a']\n",
      "(400002, 50)\n"
     ]
    }
   ],
   "source": [
    "# Code from https://tanmay17061.medium.com/load-pre-trained-glove-embeddings-in-torch-nn-embedding-layer-in-under-2-minutes-f5af8f57416a\n",
    "vocab,embeddings = [],[]\n",
    "with open('./glove/glove.6B.50d.txt','rt') as fi:\n",
    "    full_content = fi.read().strip().split('\\n')\n",
    "for i in range(len(full_content)):\n",
    "    i_word = full_content[i].split(' ')[0]\n",
    "    i_embeddings = [float(val) for val in full_content[i].split(' ')[1:]]\n",
    "    vocab.append(i_word)\n",
    "    embeddings.append(i_embeddings)\n",
    "vocab_npa = np.array(vocab)\n",
    "embs_npa = np.array(embeddings)\n",
    "vocab_npa = np.insert(vocab_npa, 0, '<pad>')\n",
    "vocab_npa = np.insert(vocab_npa, 1, '<unk>')\n",
    "print(vocab_npa[:10])\n",
    "\n",
    "pad_emb_npa = np.zeros((1,embs_npa.shape[1]))   #embedding for '<pad>' token.\n",
    "unk_emb_npa = np.mean(embs_npa,axis=0,keepdims=True)    #embedding for '<unk>' token.\n",
    "\n",
    "#insert embeddings for pad and unk tokens at top of embs_npa.\n",
    "embs_npa = np.vstack((pad_emb_npa,unk_emb_npa,embs_npa))\n",
    "print(embs_npa.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab_npa.npy','rb') as f:\n",
    "    vocab_npa = np.load(f)\n",
    "\n",
    "with open('embs_npa.npy','rb') as f:\n",
    "    emps_npa = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple model\n",
    "\n",
    "class GloveModel(nn.Module):\n",
    "    def __init__(self, embs_npa, embed_dim=50, hidden_dim=100, classes=2, threshold=0.5):\n",
    "        super(GloveModel, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding.from_pretrained(torch.from_numpy(embs_npa).float())\n",
    "        self.embedding_layer.weight.requires_grad = True\n",
    "        self.threshold = 0.5\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(hidden_dim, classes),\n",
    "            nn.Softmax(dim = 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.embedding_layer(x)\n",
    "        logits = self.classifier(embedding)\n",
    "        return torch.mean(logits, dim=1, keepdim=True).squeeze()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_dataset = IMDBDataset(\"aclImdb/train\", vocab_npa, tokenizer)\n",
    "test_dataset = IMDBDataset(\"aclImdb/test\", vocab_npa, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=100)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#schedulers\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else \"cpu\")\n",
    "def epsilon_scheduler(epsilon):\n",
    "\n",
    "    epsilon_schedule = []\n",
    "    step = epsilon / 25000\n",
    "\n",
    "    for i in range(5000):\n",
    "        epsilon_schedule.append(i * step)\n",
    "    \n",
    "    for i in range(20001):\n",
    "        epsilon_schedule.append(epsilon)\n",
    "    \n",
    "    return epsilon_schedule\n",
    "\n",
    "def kappa_scheduler():\n",
    "\n",
    "    schedule = 2001 * [1]\n",
    "    kappa_value = 1.0\n",
    "    step = 0.5/23000\n",
    "\n",
    "    for i in range(23000):\n",
    "        kappa_value -= step\n",
    "        schedule.append(kappa_value)\n",
    "    return schedule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bound_propagation(model: GloveModel, init_bounds):\n",
    "    l, u = init_bounds\n",
    "    l = l.to(device).float()\n",
    "    u = u.to(device).float()\n",
    "    bounds = [init_bounds]\n",
    "    \n",
    "    for layer in model.classifier:\n",
    "\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            new_l = 0.5 * (l + u) @ (layer.weight).T  +  layer.bias - 0.5 * (u - l) @ torch.abs(layer.weight).T # pos weights * l + neg_weights * u minimizes lower bound\n",
    "            new_u = 0.5 * (l + u) @ (layer.weight).T  +  layer.bias + 0.5 * (u - l) @ torch.abs(layer.weight).T # pos weights * l + neg_weights * u minimizes lower bound\n",
    "        if isinstance(layer, nn.ReLU):\n",
    "            new_l = l.clamp(min = 0)\n",
    "            new_u = u.clamp(min = 0)\n",
    "\n",
    "            unstable = (l < 0) & (u > 0)\n",
    "            new_l[unstable] = 0  \n",
    "        if isinstance(layer, nn.Softmax):\n",
    "            exp_lower = torch.exp(l)\n",
    "            exp_upper = torch.exp(u)\n",
    "\n",
    "            mask = torch.ones((2, 2), device=l.device)\n",
    "            mask.fill_diagonal_(0)\n",
    "            \n",
    "            sum_others_upper = torch.matmul(exp_upper, mask)  # (64, 512, 2)\n",
    "            \n",
    "            sum_others_lower = torch.matmul(exp_lower, mask)  # (64, 512, 2)\n",
    "            \n",
    "            new_l = exp_lower / (exp_lower + sum_others_upper)\n",
    "            new_u = exp_upper / (exp_upper + sum_others_lower)\n",
    "\n",
    "            new_l = new_l.mean(dim=1)\n",
    "            new_u = new_u.mean(dim=1)\n",
    "            \n",
    "        l = new_l\n",
    "        u = new_u\n",
    "        bounds.append([new_l, new_u])\n",
    "    return bounds\n",
    "\n",
    "def robust_train_loop(train_loader, model, epsilon_scheduler, kappa_scheduler, batch_counter, optimizer, attack_obj):\n",
    "    robust_err = 0\n",
    "    total_combined_loss = 0\n",
    "\n",
    "    for indices, label in tqdm(train_loader):\n",
    "        indices,label = indices.to(device), label.to(device)\n",
    "        # Fit loss \n",
    "        y_prediction = model(indices)\n",
    "        fit_loss = nn.CrossEntropyLoss()(y_prediction, label)\n",
    "\n",
    "        # Spec Loss\n",
    "        # initial_bound = torch.zeros((2, indices.shape[0], indices.shape[1], 50)).to(device)\n",
    "        initial_bound = attack_obj.get_bounds(indices, epsilon_scheduler[batch_counter])\n",
    "        bounds = bound_propagation(model, initial_bound)\n",
    "\n",
    "        lower, upper = bounds[-1]\n",
    "        cert_loss = torch.max(nn.CrossEntropyLoss()(lower, label),\n",
    "                               nn.CrossEntropyLoss()(upper, label))\n",
    "        \n",
    "        robust_preds = lower[label] < upper[label ^ 1]\n",
    "        robust_err += robust_preds.sum()\n",
    "        #combined loss\n",
    "        combined_loss = (1 - kappa_scheduler[batch_counter])*(fit_loss) + (kappa_scheduler[batch_counter])*(cert_loss)\n",
    "        total_combined_loss += combined_loss.item()\n",
    "\n",
    "        batch_counter += 1\n",
    "    \n",
    "        if optimizer:\n",
    "            optimizer.zero_grad()\n",
    "            combined_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return robust_err / len(train_loader.dataset) , total_combined_loss/len(train_loader.dataset)\n",
    "\n",
    "\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    correct = 0\n",
    "    for j, (text, labels) in enumerate(test_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        logits = model(images)\n",
    "        _, preds = torch.argmax(logits, 0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "    return correct/len(test_loader.dataset)\n",
    "\n",
    "def test_robust_model(model, test_loader, epsilon, attack_obj):\n",
    "    robust_err = 0\n",
    "    for indices, label in train_loader:\n",
    "        indices,label = indices.to(device), label.to(device)\n",
    "        initial_bound = torch.zeros((2, indices.shape[0], indices.shape[1], 50))\n",
    "        for i, index in enumerate(indices):\n",
    "            bound = attack_obj.get_bounds(index, epsilon)\n",
    "            initial_bound[:, i] = bound\n",
    "        bounds = bound_propagation(model, initial_bound)\n",
    "\n",
    "        lower, upper = bounds[-1]\n",
    "        robust_preds = lower[label] < upper[label ^ 1]\n",
    "        robust_err += robust_preds.sum()\n",
    "\n",
    "    return robust_err/test_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_obj = SubstitutionAttack(vocab_npa, embs_npa, precomputed_bounds = \"bounds_dict.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "d = attack_obj.bounds_dict\n",
    "print(type(d))\n",
    "with open(\"bounds_dict.json\", \"wb\") as f:\n",
    "    torch.save(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   \tCombined Loss\tTest Acc\tTest Robust Err\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 61/101 [58:10<38:08, 57.21s/it]\n",
      "  0%|          | 0/20 [58:10<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m)):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(t)\n\u001b[0;32m---> 21\u001b[0m     training_robust_err, combined_loss \u001b[38;5;241m=\u001b[39m \u001b[43mrobust_train_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglove\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon_schedule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkappa_schedule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_counter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattack_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     batch_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m250\u001b[39m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m24\u001b[39m:  \u001b[38;5;66;03m#decrease learning rate after 25 epochs\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[52], line 44\u001b[0m, in \u001b[0;36mrobust_train_loop\u001b[0;34m(train_loader, model, epsilon_scheduler, kappa_scheduler, batch_counter, optimizer, attack_obj)\u001b[0m\n\u001b[1;32m     41\u001b[0m robust_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     42\u001b[0m total_combined_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m indices, label \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[1;32m     45\u001b[0m     indices,label \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mto(device), label\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# Fit loss \u001b[39;00m\n",
      "File \u001b[0;32m~/hw3/.venv/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/hw3/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/hw3/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/hw3/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/hw3/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/hw3/imdb.py:37\u001b[0m, in \u001b[0;36mIMDBDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     35\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text)\n\u001b[1;32m     36\u001b[0m vocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 37\u001b[0m indices \u001b[38;5;241m=\u001b[39m [vocab\u001b[38;5;241m.\u001b[39mindex(token) \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;28;01melse\u001b[39;00m vocab\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<unk>\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# length = min(len(indices), self.max_length)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#check length:\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indices) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length:\n",
      "File \u001b[0;32m~/hw3/imdb.py:37\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     35\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text)\n\u001b[1;32m     36\u001b[0m vocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 37\u001b[0m indices \u001b[38;5;241m=\u001b[39m [\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;28;01melse\u001b[39;00m vocab\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<unk>\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# length = min(len(indices), self.max_length)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#check length:\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indices) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "glove = GloveModel(embs_npa)\n",
    "glove = glove.to(device)\n",
    "torch.manual_seed(42)\n",
    "opt = optim.Adam(glove.parameters(), lr=1e-3)\n",
    "\n",
    "EPSILON = 0.1\n",
    "EPSILON_TRAIN = 0.2\n",
    "epsilon_schedule = epsilon_scheduler(EPSILON_TRAIN)\n",
    "kappa_schedule = kappa_scheduler()\n",
    "batch_counter = 0\n",
    "\n",
    "print(\"Epoch   \", \"Combined Loss\", \"Test Acc\", \"Test Robust Err\", sep=\"\\t\")\n",
    "losses = []\n",
    "test_errs = []\n",
    "robust_errs = []\n",
    "training_robust_errs = []\n",
    "start = time.time()\n",
    "\n",
    "for t in tqdm(range(20)):\n",
    "    print(t)\n",
    "    training_robust_err, combined_loss = robust_train_loop(train_loader, glove, epsilon_schedule, kappa_schedule, batch_counter, opt, attack_obj)\n",
    "    batch_counter += 250\n",
    "    \n",
    "    if t == 24:  #decrease learning rate after 25 epochs\n",
    "        for param_group in opt.param_groups:\n",
    "            param_group[\"lr\"] = 1e-4\n",
    "\n",
    "    if t == 49:  #decrease learning rate after 49 epochs\n",
    "        for param_group in opt.param_groups:\n",
    "            param_group[\"lr\"] = 1e-5\n",
    "    \n",
    "    # test_err = test_model(glove, test_loader)\n",
    "    # robust_err = test_robust_model(glove, test_loader, EPSILON, attack_obj)\n",
    "    # test_errs.append(test_err)\n",
    "    # robust_errs.append(robust_err)\n",
    "    # print(*(\"{:.6f}\".format(i) for i in (t, combined_loss, test_err, robust_err)), sep=\"\\t\")\n",
    "    # training_robust_errs.append(training_robust_err)\n",
    "    # losses.append(combined_loss)\n",
    "\n",
    "end_time = time.time() - start\n",
    "print(f'Time: {end_time}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses\n",
    "# test_errs\n",
    "# robust_errs\n",
    "# training_robust_errs\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(losses)), losses, label = \"loss\")\n",
    "plt.plot(range(len(test_errs)), test_errs, label = \"test_err\")\n",
    "plt.plot(range(len(robust_errs)), robust_errs, label = \"robust_err\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "# plt.plot(range(len(training_robust_errs)), training_robust_errs, label=)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "## Verification\n",
    "def box_verification(model, test_loader, epsilons):\n",
    "    total_images = len(test_loader.dataset)\n",
    "    verified_robust = torch.zeros(len(epsilons))\n",
    "    adversarial_examples = {}\n",
    "\n",
    "    for idx, (indices, label) in enumerate(test_loader):\n",
    "        indices,label = indices.to(device), label.to(device)\n",
    "        robust_err = 0\n",
    "        for epsilon in epsilons:\n",
    "            initial_bound = torch.zeros((2, indices.shape[0], indices.shape[1], 50))\n",
    "            for i, index in enumerate(indices):\n",
    "                bound = attack_obj.get_bounds(index, epsilon)\n",
    "                initial_bound[:, i] = bound\n",
    "            bounds = bound_propagation(model, initial_bound)\n",
    "\n",
    "            lower, upper = bounds[-1]\n",
    "            robust_preds = lower[label] < upper[label ^ 1]\n",
    "            robust_err += robust_preds.sum()\n",
    "        verified_robust[idx] = robust_err / len(test_loader.dataset)\n",
    "\n",
    "    return verified_robust\n",
    "\n",
    "epsilons = np.linspace(0.01, 0.1, 10)\n",
    "verified_accuracy, adversarial_examples = box_verification(glove, test_loader, epsilons)\n",
    "\n",
    "for e, a in zip(epsilons, verified_accuracy):\n",
    "    print(f'Accuracy for {e}: {a}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.classifier[0].weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
