{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"/home/oru2/project/project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "# import matplotlib.pyplot as plt\n",
    "import attacks\n",
    "from privacy_accountant import PrivacyAccountant\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../imdb_data/IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/oru2/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/oru2/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/oru2/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 40000, Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "#credit: https://www.kaggle.com/code/m0hammadjavad/imdb-sentiment-classifier-pytorch/notebook\n",
    "import nltk\n",
    "import os\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "\n",
    "    doc = nlp(' '.join(filtered_tokens))\n",
    "    \n",
    "    lemmetized_tokens = [token.lemma_ for token in doc]\n",
    "    \n",
    "    return ' '.join(lemmetized_tokens)\n",
    "\n",
    "if os.path.exists(\"../imdb_data/IMDB Dataset_with_cleaned_reviews.csv\"):\n",
    "    data = pd.read_csv(\"../imdb_data/IMDB Dataset_with_cleaned_reviews.csv\")\n",
    "else:  \n",
    "    data[\"cleaned_reviews\"] = data[\"review\"].apply(preprocess_text)\n",
    "    data.to_csv(\"../imdb_data/IMDB Dataset_with_cleaned_reviews.csv\")\n",
    "\n",
    "data[\"sentiment\"] = data[\"sentiment\"].map({\"positive\": 1, \"negative\": 0})\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[\"cleaned_reviews\"], data[\"sentiment\"], test_size=0.2, random_state=42)\n",
    "print(f'Training samples: {len(X_train)}, Test samples: {len(X_test)}')\n",
    "\n",
    "\n",
    "# Create a vocabulary based on the training data\n",
    "def build_vocab(texts):\n",
    "    \n",
    "    # this makes a dict with unique words and their count as the value\n",
    "    # although this is not going to be used directly, it only gives us unique words without repeatition\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        counter.update(text.split())\n",
    "        \n",
    "    # this makes a dict of unique words and their index as the value\n",
    "    vocab = {word: idx for idx, (word, _) in enumerate(counter.items(), 1)}  # Reserve index 0 for padding\n",
    "    \n",
    "    # this is a convention which is going to be used to convert batches to a fixed size\n",
    "    vocab['<PAD>'] = 0\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "# Build the vocabulary\n",
    "vocab = build_vocab(X_train)\n",
    "\n",
    "# Encode text sequences into integer sequences\n",
    "def encode_text(text, vocab, max_length=200):\n",
    "    tokens = text.split()\n",
    "    encoded = [vocab.get(token, 0) for token in tokens]  # 0 for unknown tokens\n",
    "    if len(encoded) < max_length:\n",
    "        encoded += [vocab['<PAD>']] * (max_length - len(encoded))  # Padding\n",
    "    return encoded[:max_length]  # Truncate to max_length\n",
    "\n",
    "# Encode all reviews\n",
    "X_train_encoded = torch.tensor([encode_text(text, vocab) for text in X_train])\n",
    "X_test_encoded = torch.tensor([encode_text(text, vocab) for text in X_test])\n",
    "y_train_tensor = torch.nn.functional.one_hot(torch.tensor(y_train.values))\n",
    "y_test_tensor =  torch.nn.functional.one_hot(torch.tensor(y_test.values))\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_data = TensorDataset(X_train_encoded, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_encoded, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6b17bf66d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "batch_size = 100\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, num_epochs):\n",
    "    # TODO: implement this function that trains a given model on the MNIST dataset.\n",
    "    # this is a general-purpose function for both standard training and adversarial training.\n",
    "    # (toggle enable_defense parameter to switch between training schemes)\n",
    "    model.train()\n",
    "    # epsilons_clean = []\n",
    "    lr = 1e-2\n",
    "    losses = []\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        for index, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = model(inputs)\n",
    "            optimizer.zero_grad()\n",
    "            loss = nn.BCELoss()(logits.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "                \n",
    "                \n",
    "\n",
    "        print(f'Epoch [{epoch}/{num_epochs}] Loss = {loss.item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import SentimentModel\n",
    "vocab_size = len(vocab)\n",
    "embed_size = 100\n",
    "hidden_size = 128\n",
    "output_size = 2  # Binary classification (positive/negative)\n",
    "num_layers = 2\n",
    "fc_model = SentimentModel(vocab_size, embed_size, hidden_size, output_size, num_layers).to(device)\n",
    "num_epochs = 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:03<01:10,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/20] Loss = 0.692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:05<00:48,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] Loss = 0.683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:07<00:40,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20] Loss = 0.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:09<00:35,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20] Loss = 0.340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:11<00:31,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20] Loss = 0.295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:13<00:29,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20] Loss = 0.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:15<00:26,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20] Loss = 0.071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [00:17<00:24,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20] Loss = 0.104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [00:19<00:22,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20] Loss = 0.109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [00:21<00:19,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20] Loss = 0.181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [00:23<00:17,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20] Loss = 0.052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [00:25<00:15,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20] Loss = 0.031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [00:27<00:13,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20] Loss = 0.056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [00:29<00:11,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20] Loss = 0.028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [00:31<00:09,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20] Loss = 0.066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [00:33<00:08,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20] Loss = 0.029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [00:35<00:06,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20] Loss = 0.013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [00:37<00:03,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20] Loss = 0.103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [00:39<00:01,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20] Loss = 0.049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:41<00:00,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20] Loss = 0.026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(fc_model, train_loader, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(fc_model, 'models/baseline.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_834903/2272887102.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  fc_model = torch.load(f=\"models/baseline.pt\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "PytorchStreamReader failed locating file data/0: file not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fc_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/baseline.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project/env/lib/python3.10/site-packages/torch/serialization.py:1360\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1358\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1359\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1360\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1368\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/project/env/lib/python3.10/site-packages/torch/serialization.py:1848\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[1;32m   1847\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 1848\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1849\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1851\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m~/project/env/lib/python3.10/site-packages/torch/serialization.py:1812\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1810\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1811\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1812\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1813\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1814\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1816\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/project/env/lib/python3.10/site-packages/torch/serialization.py:1772\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1769\u001b[0m     storage \u001b[38;5;241m=\u001b[39m overall_storage[storage_offset : storage_offset \u001b[38;5;241m+\u001b[39m numel]\n\u001b[1;32m   1770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1771\u001b[0m     storage \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1772\u001b[0m         \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1773\u001b[0m         \u001b[38;5;241m.\u001b[39m_typed_storage()\n\u001b[1;32m   1774\u001b[0m         \u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[1;32m   1775\u001b[0m     )\n\u001b[1;32m   1776\u001b[0m \u001b[38;5;66;03m# swap here if byteswapping is needed\u001b[39;00m\n\u001b[1;32m   1777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m byteorderdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed locating file data/0: file not found"
     ]
    }
   ],
   "source": [
    "fc_model = torch.load(f=\"models/baseline.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 84.73%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "fc_model.eval()\n",
    "for j, (inputs, labels) in enumerate(test_loader):\n",
    "  inputs = inputs.to(device)\n",
    "  labels = labels.to(device)\n",
    "  \n",
    "  logits = fc_model(inputs)\n",
    "\n",
    "  prediction = torch.argmax(logits, 1)\n",
    "  correct += (prediction == torch.argmax(labels, dim=1)).sum().item()\n",
    "  # print('Batch [{}/{}]'.format(j+1, len(test_loader)))\n",
    "fc_model.train()\n",
    "print('Accuracy = {}%'.format(float(correct) * 100 / 10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 50.9%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "eps = 0.1\n",
    "# attack = attacks.LSTMPGD(fc_model, epsilon=eps, num_steps=10, embedding_layer=fc_model.embedding)\n",
    "torch.backends.cudnn.enabled = False\n",
    "for j, (inputs, labels) in enumerate(test_loader):\n",
    "  inputs = inputs.to(device)\n",
    "  labels = labels.to(device)\n",
    "  embeddings = fc_model.embedding(inputs)\n",
    "  adv_inputs = attacks.perturb_data(fc_model, embeddings, labels, epsilon = eps)\n",
    "  # print(adv_images)\n",
    "  logits = fc_model(inputs)\n",
    "\n",
    "  adv_logits = fc_model.lstm(adv_inputs)[0][:, -1, :]\n",
    "  adv_logits = torch.sigmoid(fc_model.fc(adv_logits))\n",
    "  # adv_logits = fc_model(adv_images)\n",
    "\n",
    "  prediction = torch.argmax(logits, 1)\n",
    "\n",
    "  adv_prediction = torch.argmax(adv_logits, 1)\n",
    "\n",
    "\n",
    "  correct += (prediction == torch.argmax(labels)).sum().item()\n",
    "  correct += (adv_prediction == torch.argmax(labels)).sum().item()\n",
    "  # print('Batch [{}/{}]'.format(j+1, len(test_loader)))\n",
    "print('Accuracy = {}%'.format(float(correct) * 100 / 20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.attacks.inference.membership_inference import MembershipInferenceBlackBox\n",
    "from art.estimators.classification import PyTorchClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import SentimentModelNoEmbed\n",
    "no_embed_model = SentimentModelNoEmbed(embed_size, hidden_size, output_size, num_layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['embedding.weight', 'lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'lstm.weight_ih_l1', 'lstm.weight_hh_l1', 'lstm.bias_ih_l1', 'lstm.bias_hh_l1', 'fc.weight', 'fc.bias'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(fc_model.state_dict().keys())\n",
    "state_dict = {k: fc_model.state_dict()[k] for k in filter(lambda x: not x.startswith('embedding'), fc_model.state_dict())}\n",
    "no_embed_model.load_state_dict(state_dict, strict=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(fc_model.parameters())\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "# Wrap the PyTorch model in ART's PyTorchClassifier\n",
    "art_classifier = PyTorchClassifier(\n",
    "    model=no_embed_model,\n",
    "    loss=criterion,\n",
    "    optimizer=optimizer,\n",
    "    input_shape=(200,),\n",
    "    nb_classes=2\n",
    ")\n",
    "attack_train_size = 10000\n",
    "attack_test_size = 5000\n",
    "\n",
    "x_train = train_data.tensors[0]\n",
    "y_train = train_data.tensors[1].detach().numpy()\n",
    "\n",
    "\n",
    "x_test = test_data.tensors[0]\n",
    "y_test = test_data.tensors[1].detach().numpy()\n",
    "\n",
    "x_train = nn.Embedding(vocab_size, embed_size)(x_train).detach().numpy()\n",
    "x_test = nn.Embedding(vocab_size, embed_size)(x_test).detach().numpy()\n",
    "\n",
    "attack = MembershipInferenceBlackBox(estimator=art_classifier, attack_model_type=\"nn\")\n",
    "attack.fit(x_train[:attack_train_size], y_train[:attack_train_size], x_test[:attack_test_size], y_test[:attack_test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Members Accuracy: 0.8577\n",
      "Non Members Accuracy 0.4772\n",
      "Attack Accuracy 0.8033\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "mlp_inferred_train_bb = attack.infer(x_train[attack_train_size:], y_train[attack_train_size:])\n",
    "mlp_inferred_test_bb = attack.infer(x_test[attack_test_size:], y_test[attack_test_size:])\n",
    "\n",
    "# check accuracy\n",
    "mlp_train_acc_bb = np.sum(mlp_inferred_train_bb) / len(mlp_inferred_train_bb)\n",
    "mlp_test_acc_bb = 1 - (np.sum(mlp_inferred_test_bb) / len(mlp_inferred_test_bb))\n",
    "mlp_acc_bb = (mlp_train_acc_bb * len(mlp_inferred_train_bb) + mlp_test_acc_bb * len(mlp_inferred_test_bb)) / (len(mlp_inferred_train_bb) + len(mlp_inferred_test_bb))\n",
    "\n",
    "print(f\"Members Accuracy: {mlp_train_acc_bb:.4f}\")\n",
    "print(f\"Non Members Accuracy {mlp_test_acc_bb:.4f}\")\n",
    "print(f\"Attack Accuracy {mlp_acc_bb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.2008e+00,  1.5396e+00,  1.4179e+00,  ...,  5.5041e-01,\n",
       "          -4.3193e-01, -2.4356e-01],\n",
       "         [ 2.2167e-01, -2.7137e-01,  2.1014e-01,  ..., -1.0977e+00,\n",
       "          -7.6189e-01, -1.6361e+00],\n",
       "         [-8.4064e-03, -9.4465e-02, -4.9066e-01,  ...,  7.0453e-01,\n",
       "           6.9287e-01, -1.4205e+00],\n",
       "         ...,\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01],\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01],\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01]],\n",
       "\n",
       "        [[ 1.4441e+00,  1.1737e+00, -1.4373e+00,  ..., -1.0749e+00,\n",
       "          -1.6892e+00, -1.3481e+00],\n",
       "         [ 6.1772e-01, -6.9171e-01,  1.2618e+00,  ...,  1.9872e+00,\n",
       "          -3.9961e-01, -5.8853e-01],\n",
       "         [-1.2826e-01, -4.8372e-01, -5.2664e-01,  ..., -5.0066e-01,\n",
       "          -1.2826e-01, -1.1033e+00],\n",
       "         ...,\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01],\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01],\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01]],\n",
       "\n",
       "        [[ 3.5313e-01, -6.4696e-02, -1.0170e+00,  ..., -4.6488e-01,\n",
       "          -5.7841e-02,  1.0938e+00],\n",
       "         [-1.1779e+00, -1.1605e+00,  2.1275e-01,  ...,  5.9294e-01,\n",
       "           1.0522e+00,  1.0124e+00],\n",
       "         [ 6.7079e-01, -5.0549e-01, -2.4617e-01,  ...,  7.1658e-01,\n",
       "           1.3217e+00, -9.9319e-01],\n",
       "         ...,\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01],\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01],\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-4.4379e-01,  6.4212e-02, -5.5830e-01,  ..., -1.7309e-01,\n",
       "          -6.6929e-01,  4.9953e-01],\n",
       "         [-3.8706e-02,  2.0035e-01, -4.2953e-01,  ...,  9.0817e-01,\n",
       "           2.6609e+00, -8.8467e-03],\n",
       "         [-1.0066e-01,  6.7764e-01,  1.3813e+00,  ...,  6.0390e-01,\n",
       "          -1.0233e-01,  4.5856e-02],\n",
       "         ...,\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01],\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01],\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01]],\n",
       "\n",
       "        [[ 4.2877e-01,  6.3098e-01,  8.5166e-01,  ..., -2.6516e-03,\n",
       "           3.6147e-01, -1.2036e-01],\n",
       "         [-5.8604e-01,  5.1376e-01,  7.6174e-01,  ...,  1.1001e-01,\n",
       "           5.1262e-01,  5.7579e-01],\n",
       "         [ 1.4745e+00, -7.0118e-01, -3.9411e-01,  ..., -7.4156e-01,\n",
       "           5.4309e-01, -1.1444e+00],\n",
       "         ...,\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01],\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01],\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01]],\n",
       "\n",
       "        [[-1.6000e+00,  3.5574e-01,  4.0358e-01,  ...,  8.7753e-01,\n",
       "           1.2214e+00,  1.5603e+00],\n",
       "         [-1.2826e-01, -4.8372e-01, -5.2664e-01,  ..., -5.0066e-01,\n",
       "          -1.2826e-01, -1.1033e+00],\n",
       "         [ 7.4236e-01,  2.9955e-01,  8.0509e-01,  ...,  1.3728e+00,\n",
       "           2.5560e-01,  5.1105e-01],\n",
       "         ...,\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01],\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01],\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01]]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
