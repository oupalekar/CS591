{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"/home/oru2/project/project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "# import matplotlib.pyplot as plt\n",
    "import attacks\n",
    "from privacy_accountant import PrivacyAccountant\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../imdb_data/IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/oru2/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/oru2/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/oru2/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 40000, Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "#credit: https://www.kaggle.com/code/m0hammadjavad/imdb-sentiment-classifier-pytorch/notebook\n",
    "import nltk\n",
    "import os\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "\n",
    "    doc = nlp(' '.join(filtered_tokens))\n",
    "    \n",
    "    lemmetized_tokens = [token.lemma_ for token in doc]\n",
    "    \n",
    "    return ' '.join(lemmetized_tokens)\n",
    "\n",
    "if os.path.exists(\"../imdb_data/IMDB Dataset_with_cleaned_reviews.csv\"):\n",
    "    data = pd.read_csv(\"../imdb_data/IMDB Dataset_with_cleaned_reviews.csv\")\n",
    "else:  \n",
    "    data[\"cleaned_reviews\"] = data[\"review\"].apply(preprocess_text)\n",
    "    data.to_csv(\"../imdb_data/IMDB Dataset_with_cleaned_reviews.csv\")\n",
    "\n",
    "data[\"sentiment\"] = data[\"sentiment\"].map({\"positive\": 1, \"negative\": 0})\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[\"cleaned_reviews\"], data[\"sentiment\"], test_size=0.2, random_state=42)\n",
    "print(f'Training samples: {len(X_train)}, Test samples: {len(X_test)}')\n",
    "\n",
    "\n",
    "# Create a vocabulary based on the training data\n",
    "def build_vocab(texts):\n",
    "    \n",
    "    # this makes a dict with unique words and their count as the value\n",
    "    # although this is not going to be used directly, it only gives us unique words without repeatition\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        counter.update(text.split())\n",
    "        \n",
    "    # this makes a dict of unique words and their index as the value\n",
    "    vocab = {word: idx for idx, (word, _) in enumerate(counter.items(), 1)}  # Reserve index 0 for padding\n",
    "    \n",
    "    # this is a convention which is going to be used to convert batches to a fixed size\n",
    "    vocab['<PAD>'] = 0\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "# Build the vocabulary\n",
    "vocab = build_vocab(X_train)\n",
    "\n",
    "# Encode text sequences into integer sequences\n",
    "def encode_text(text, vocab, max_length=200):\n",
    "    tokens = text.split()\n",
    "    encoded = [vocab.get(token, 0) for token in tokens]  # 0 for unknown tokens\n",
    "    if len(encoded) < max_length:\n",
    "        encoded += [vocab['<PAD>']] * (max_length - len(encoded))  # Padding\n",
    "    return encoded[:max_length]  # Truncate to max_length\n",
    "\n",
    "# Encode all reviews\n",
    "X_train_encoded = torch.tensor([encode_text(text, vocab) for text in X_train])\n",
    "X_test_encoded = torch.tensor([encode_text(text, vocab) for text in X_test])\n",
    "y_train_tensor = torch.nn.functional.one_hot(torch.tensor(y_train.values))\n",
    "y_test_tensor =  torch.nn.functional.one_hot(torch.tensor(y_test.values))\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_data = TensorDataset(X_train_encoded, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_encoded, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=500, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f56efc82270>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "batch_size = 500\n",
    "\n",
    "MAX_GRAD_NORM = 1.2\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import SentimentModel\n",
    "vocab_size = len(vocab)\n",
    "embed_size = 100\n",
    "hidden_size = 128\n",
    "output_size = 2  # Binary classification (positive/negative)\n",
    "num_layers = 2\n",
    "del fc_model\n",
    "fc_model = SentimentModel(vocab_size, embed_size, hidden_size, output_size, num_layers).to(device)\n",
    "num_epochs = 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oru2/project/env/lib/python3.10/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "/home/oru2/project/env/lib/python3.10/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from opacus import PrivacyEngine\n",
    "model.remove_hooks()\n",
    "privacy_engine = PrivacyEngine(secure_mode=False)\n",
    "optimizer = torch.optim.SGD(fc_model.parameters(), lr=1e-2)\n",
    "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
    "    module=fc_model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_loader,\n",
    "    max_grad_norm=1.5,\n",
    "    target_delta=1e-5,\n",
    "    target_epsilon=5,\n",
    "    epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, train_loader, num_epochs):\n",
    "    # TODO: implement this function that trains a given model on the MNIST dataset.\n",
    "    # this is a general-purpose function for both standard training and adversarial training.\n",
    "    # (toggle enable_defense parameter to switch between training schemes)\n",
    "    noise_multiplier = 1.1\n",
    "    lr = 1e-2\n",
    "    losses = []\n",
    "    model.train()\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        running_loss = 0.0\n",
    "        for index, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass (Standard Training)\n",
    "            outputs = model(inputs)\n",
    "            loss = nn.BCELoss(reduction='none')(outputs.squeeze(), labels.float())\n",
    "            loss = loss.mean()\n",
    "            # Adversarial Training (if enabled)\n",
    "            with torch.no_grad():\n",
    "                embeddings = model.embedding(inputs)  # Embedding layer forward pass\n",
    "            adv_inputs = attacks.perturb_data(model, embeddings, labels)  # Generate adversarial inputs\n",
    "\n",
    "            predictions = model.lstm(adv_inputs)[0][:, -1, :]  # LSTM output\n",
    "            predictions = torch.sigmoid(model.fc(predictions))  # Final prediction layer\n",
    "            adv_loss = nn.BCELoss(reduction='none')(predictions.squeeze(), labels.float())\n",
    "            adv_loss = adv_loss.mean()\n",
    "\n",
    "\n",
    "            total_loss = loss + 0.5 * adv_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update running loss\n",
    "            running_loss += total_loss.item()\n",
    "\n",
    "        \n",
    "        printstr = (\n",
    "        f\"\\t Epoch {epoch}. Loss: {np.mean(running_loss):.6f}\"\n",
    "        )\n",
    "        if privacy_engine:\n",
    "            epsilon = privacy_engine.get_epsilon(1e-5)\n",
    "            printstr += f\" | (ε = {epsilon:.2f}, δ = {1e-5})\"\n",
    "\n",
    "        print(printstr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/home/oru2/project/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "  5%|▌         | 1/20 [01:57<37:18, 117.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Epoch 0. Loss: 83.940964 | (ε = 1.76, δ = 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [03:52<34:48, 116.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Epoch 1. Loss: 83.808544 | (ε = 2.10, δ = 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [05:52<33:24, 117.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Epoch 2. Loss: 83.716841 | (ε = 2.36, δ = 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [07:47<31:08, 116.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Epoch 3. Loss: 83.637325 | (ε = 2.59, δ = 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [09:41<28:56, 115.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Epoch 4. Loss: 83.584358 | (ε = 2.79, δ = 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [11:35<26:51, 115.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Epoch 5. Loss: 83.536553 | (ε = 2.98, δ = 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [13:30<24:53, 114.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Epoch 6. Loss: 83.500777 | (ε = 3.16, δ = 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [15:24<22:55, 114.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Epoch 7. Loss: 83.475892 | (ε = 3.34, δ = 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [17:25<21:24, 116.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Epoch 8. Loss: 83.454510 | (ε = 3.50, δ = 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [19:19<19:18, 115.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Epoch 9. Loss: 83.429339 | (ε = 3.66, δ = 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [21:19<17:33, 117.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Epoch 10. Loss: 83.411898 | (ε = 3.81, δ = 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [23:13<15:29, 116.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Epoch 11. Loss: 83.397715 | (ε = 3.95, δ = 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [25:08<13:31, 115.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Epoch 12. Loss: 83.384014 | (ε = 4.10, δ = 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [27:03<11:33, 115.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Epoch 13. Loss: 83.369742 | (ε = 4.23, δ = 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [28:59<09:39, 115.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Epoch 14. Loss: 83.354959 | (ε = 4.37, δ = 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [30:55<07:42, 115.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Epoch 15. Loss: 83.356992 | (ε = 4.50, δ = 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [32:51<05:47, 115.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Epoch 16. Loss: 83.340416 | (ε = 4.63, δ = 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [34:45<03:50, 115.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Epoch 17. Loss: 83.335898 | (ε = 4.75, δ = 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [36:39<01:55, 115.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Epoch 18. Loss: 83.327679 | (ε = 4.88, δ = 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [38:34<00:00, 115.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Epoch 19. Loss: 83.318122 | (ε = 5.00, δ = 1e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model, optimizer, train_loader, num_epochs)\n",
    "# train_model_dp(fc_model, train_loader, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'models/dp-adv.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1187474/4042864840.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  fc_model = torch.load(f=\"models/dp.pt\")\n"
     ]
    }
   ],
   "source": [
    "fc_model = torch.load(f=\"models/dp.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 50.68%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "model.eval()\n",
    "for j, (inputs, labels) in enumerate(test_loader):\n",
    "  inputs = inputs.to(device)\n",
    "  labels = labels.to(device)\n",
    "  \n",
    "  logits = model(inputs)\n",
    "\n",
    "  prediction = torch.argmax(logits, 1)\n",
    "  correct += (prediction == torch.argmax(labels, dim=1)).sum().item()\n",
    "  # print('Batch [{}/{}]'.format(j+1, len(test_loader)))\n",
    "model.train()\n",
    "print('Accuracy = {}%'.format(float(correct) * 100 / 10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "pop from empty list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      9\u001b[0m   embeddings \u001b[38;5;241m=\u001b[39m fc_model\u001b[38;5;241m.\u001b[39membedding(inputs)\n\u001b[0;32m---> 10\u001b[0m adv_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mattacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperturb_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfc_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# print(adv_images)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m logits \u001b[38;5;241m=\u001b[39m fc_model(inputs)\n",
      "File \u001b[0;32m~/project/project/attacks.py:182\u001b[0m, in \u001b[0;36mperturb_data\u001b[0;34m(model, X_test, label, optimizer, epsilon)\u001b[0m\n\u001b[1;32m    179\u001b[0m predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(model\u001b[38;5;241m.\u001b[39mfc(predictions))\n\u001b[1;32m    180\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy(predictions\u001b[38;5;241m.\u001b[39msqueeze(), label\u001b[38;5;241m.\u001b[39mfloat())\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 182\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m grad \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mgrad\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# # Apply perturbation\u001b[39;00m\n",
      "File \u001b[0;32m~/project/env/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project/env/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project/env/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[0;32m~/project/env/lib/python3.10/site-packages/torch/nn/modules/module.py:98\u001b[0m, in \u001b[0;36m_WrappedHook.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to call the hook of a dead Module!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/project/env/lib/python3.10/site-packages/opacus/grad_sample/grad_sample_module.py:328\u001b[0m, in \u001b[0;36mGradSampleModule.capture_backprops_hook\u001b[0;34m(self, module, _forward_input, forward_output, loss_reduction, batch_first)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    327\u001b[0m backprops \u001b[38;5;241m=\u001b[39m forward_output[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m--> 328\u001b[0m activations, backprops \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrearrange_grad_samples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackprops\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackprops\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_reduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_reduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforce_functorch \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(module) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGRAD_SAMPLERS:\n\u001b[1;32m    335\u001b[0m     grad_sampler_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGRAD_SAMPLERS[\u001b[38;5;28mtype\u001b[39m(module)]\n",
      "File \u001b[0;32m~/project/env/lib/python3.10/site-packages/opacus/grad_sample/grad_sample_module.py:398\u001b[0m, in \u001b[0;36mGradSampleModule.rearrange_grad_samples\u001b[0;34m(self, module, backprops, loss_reduction, batch_first)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_batch_len\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;66;03m# For packed sequences, max_batch_len is set in the forward of the model (e.g. the LSTM)\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;66;03m# Otherwise we infer it here\u001b[39;00m\n\u001b[1;32m    394\u001b[0m     module\u001b[38;5;241m.\u001b[39mmax_batch_len \u001b[38;5;241m=\u001b[39m _get_batch_size(\n\u001b[1;32m    395\u001b[0m         module\u001b[38;5;241m=\u001b[39mmodule,\n\u001b[1;32m    396\u001b[0m         batch_dim\u001b[38;5;241m=\u001b[39mbatch_dim,\n\u001b[1;32m    397\u001b[0m     )\n\u001b[0;32m--> 398\u001b[0m activations \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m n \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mmax_batch_len\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss_reduction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from empty list"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "eps = 0.1\n",
    "# attack = attacks.LSTMPGD(fc_model, epsilon=eps, num_steps=10, embedding_layer=fc_model.embedding)\n",
    "fc_model.eval()\n",
    "for j, (inputs, labels) in enumerate(test_loader):\n",
    "  inputs = inputs.to(device)\n",
    "  labels = labels.to(device)\n",
    "  with torch.no_grad():\n",
    "    embeddings = fc_model.embedding(inputs)\n",
    "  adv_inputs = attacks.perturb_data(fc_model, embeddings, labels, optimizer)\n",
    "  # print(adv_images)\n",
    "  logits = fc_model(inputs)\n",
    "\n",
    "  # adv_logits = model.lstm(adv_inputs)[0][:, -1, :]\n",
    "  # adv_logits = torch.sigmoid(fc_model.fc(adv_logits))\n",
    "  # adv_logits = fc_model(adv_images)\n",
    "\n",
    "  prediction = torch.argmax(logits, 1)\n",
    "\n",
    "  # adv_prediction = torch.argmax(adv_logits, 1)\n",
    "\n",
    "\n",
    "  correct += (prediction == torch.argmax(labels, dim = 1)).sum().item()\n",
    "  # correct += (adv_prediction == torch.argmax(labels)).sum().item()\n",
    "  # print('Batch [{}/{}]'.format(j+1, len(test_loader)))\n",
    "print('Accuracy = {}%'.format(float(correct) * 100 / 20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.attacks.inference.membership_inference import MembershipInferenceBlackBox\n",
    "from art.estimators.classification import PyTorchClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oru2/project/env/lib/python3.10/site-packages/opacus/privacy_engine.py:469: UserWarning: You're calling make_private_with_epsilon with non-zero privacy budget already spent. Returned noise_multiplier assumes zero starting point, so your overall privacy budget will be higher.\n",
      "  warnings.warn(\n",
      "/home/oru2/project/env/lib/python3.10/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from model import SentimentModelNoEmbed\n",
    "no_embed_model = SentimentModelNoEmbed(embed_size, hidden_size, output_size, num_layers).to(device)\n",
    "\n",
    "optimizer_ = torch.optim.Adam(no_embed_model.parameters(), lr=1e-2)\n",
    "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
    "    module=no_embed_model,\n",
    "    optimizer=optimizer_,\n",
    "    data_loader=train_loader,\n",
    "    max_grad_norm=1.5,\n",
    "    target_delta=1e-5,\n",
    "    target_epsilon=5,\n",
    "    epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['_module.lstm.weight_ih_l0', '_module.lstm.bias_ih_l0', '_module.lstm.weight_hh_l0', '_module.lstm.bias_hh_l0', '_module.lstm.weight_ih_l1', '_module.lstm.bias_ih_l1', '_module.lstm.weight_hh_l1', '_module.lstm.bias_hh_l1', '_module.lstm.l0.ih.weight', '_module.lstm.l0.ih.bias', '_module.lstm.l0.hh.weight', '_module.lstm.l0.hh.bias', '_module.lstm.l1.ih.weight', '_module.lstm.l1.ih.bias', '_module.lstm.l1.hh.weight', '_module.lstm.l1.hh.bias', '_module.fc.weight', '_module.fc.bias'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.state_dict().keys())\n",
    "state_dict = {k: fc_model.state_dict()[k] for k in filter(lambda x: not 'embedding' in x, fc_model.state_dict())}\n",
    "no_embed_model.load_state_dict(state_dict, strict=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "# Wrap the PyTorch model in ART's PyTorchClassifier\n",
    "art_classifier = PyTorchClassifier(\n",
    "    model=no_embed_model,\n",
    "    loss=criterion,\n",
    "    optimizer=optimizer_,\n",
    "    input_shape=(200,),\n",
    "    nb_classes=2\n",
    ")\n",
    "attack_train_size = 10000\n",
    "attack_test_size = 5000\n",
    "\n",
    "x_train = train_data.tensors[0]\n",
    "y_train = train_data.tensors[1].detach().numpy()\n",
    "\n",
    "\n",
    "x_test = test_data.tensors[0]\n",
    "y_test = test_data.tensors[1].detach().numpy()\n",
    "\n",
    "x_train = nn.Embedding(vocab_size, embed_size)(x_train).detach().numpy()\n",
    "x_test = nn.Embedding(vocab_size, embed_size)(x_test).detach().numpy()\n",
    "\n",
    "attack = MembershipInferenceBlackBox(estimator=art_classifier, attack_model_type=\"nn\")\n",
    "attack.fit(x_train[:attack_train_size], y_train[:attack_train_size], x_test[:attack_test_size], y_test[:attack_test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Members Accuracy: 0.9982\n",
      "Non Members Accuracy 0.8660\n",
      "Attack Accuracy 0.9793\n"
     ]
    }
   ],
   "source": [
    "mlp_inferred_train_bb = attack.infer(x_train[attack_train_size:], y_train[attack_train_size:])\n",
    "mlp_inferred_test_bb = attack.infer(x_test[attack_test_size:], y_test[attack_test_size:])\n",
    "\n",
    "# check accuracy\n",
    "mlp_train_acc_bb = np.sum(mlp_inferred_train_bb) / len(mlp_inferred_train_bb)\n",
    "mlp_test_acc_bb = 1 - (np.sum(mlp_inferred_test_bb) / len(mlp_inferred_test_bb))\n",
    "mlp_acc_bb = (mlp_train_acc_bb * len(mlp_inferred_train_bb) + mlp_test_acc_bb * len(mlp_inferred_test_bb)) / (len(mlp_inferred_train_bb) + len(mlp_inferred_test_bb))\n",
    "\n",
    "print(f\"Members Accuracy: {mlp_train_acc_bb:.4f}\")\n",
    "print(f\"Non Members Accuracy {mlp_test_acc_bb:.4f}\")\n",
    "print(f\"Attack Accuracy {mlp_acc_bb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "privacy_engine.get_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.2008e+00,  1.5396e+00,  1.4179e+00,  ...,  5.5041e-01,\n",
       "          -4.3193e-01, -2.4356e-01],\n",
       "         [ 2.2167e-01, -2.7137e-01,  2.1014e-01,  ..., -1.0977e+00,\n",
       "          -7.6189e-01, -1.6361e+00],\n",
       "         [-8.4064e-03, -9.4465e-02, -4.9066e-01,  ...,  7.0453e-01,\n",
       "           6.9287e-01, -1.4205e+00],\n",
       "         ...,\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01],\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01],\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01]],\n",
       "\n",
       "        [[ 1.4441e+00,  1.1737e+00, -1.4373e+00,  ..., -1.0749e+00,\n",
       "          -1.6892e+00, -1.3481e+00],\n",
       "         [ 6.1772e-01, -6.9171e-01,  1.2618e+00,  ...,  1.9872e+00,\n",
       "          -3.9961e-01, -5.8853e-01],\n",
       "         [-1.2826e-01, -4.8372e-01, -5.2664e-01,  ..., -5.0066e-01,\n",
       "          -1.2826e-01, -1.1033e+00],\n",
       "         ...,\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01],\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01],\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01]],\n",
       "\n",
       "        [[ 3.5313e-01, -6.4696e-02, -1.0170e+00,  ..., -4.6488e-01,\n",
       "          -5.7841e-02,  1.0938e+00],\n",
       "         [-1.1779e+00, -1.1605e+00,  2.1275e-01,  ...,  5.9294e-01,\n",
       "           1.0522e+00,  1.0124e+00],\n",
       "         [ 6.7079e-01, -5.0549e-01, -2.4617e-01,  ...,  7.1658e-01,\n",
       "           1.3217e+00, -9.9319e-01],\n",
       "         ...,\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01],\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01],\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-4.4379e-01,  6.4212e-02, -5.5830e-01,  ..., -1.7309e-01,\n",
       "          -6.6929e-01,  4.9953e-01],\n",
       "         [-3.8706e-02,  2.0035e-01, -4.2953e-01,  ...,  9.0817e-01,\n",
       "           2.6609e+00, -8.8467e-03],\n",
       "         [-1.0066e-01,  6.7764e-01,  1.3813e+00,  ...,  6.0390e-01,\n",
       "          -1.0233e-01,  4.5856e-02],\n",
       "         ...,\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01],\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01],\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01]],\n",
       "\n",
       "        [[ 4.2877e-01,  6.3098e-01,  8.5166e-01,  ..., -2.6516e-03,\n",
       "           3.6147e-01, -1.2036e-01],\n",
       "         [-5.8604e-01,  5.1376e-01,  7.6174e-01,  ...,  1.1001e-01,\n",
       "           5.1262e-01,  5.7579e-01],\n",
       "         [ 1.4745e+00, -7.0118e-01, -3.9411e-01,  ..., -7.4156e-01,\n",
       "           5.4309e-01, -1.1444e+00],\n",
       "         ...,\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01],\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01],\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01]],\n",
       "\n",
       "        [[-1.6000e+00,  3.5574e-01,  4.0358e-01,  ...,  8.7753e-01,\n",
       "           1.2214e+00,  1.5603e+00],\n",
       "         [-1.2826e-01, -4.8372e-01, -5.2664e-01,  ..., -5.0066e-01,\n",
       "          -1.2826e-01, -1.1033e+00],\n",
       "         [ 7.4236e-01,  2.9955e-01,  8.0509e-01,  ...,  1.3728e+00,\n",
       "           2.5560e-01,  5.1105e-01],\n",
       "         ...,\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01],\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01],\n",
       "         [ 7.2725e-02,  7.8139e-01,  6.2422e-01,  ..., -2.6550e+00,\n",
       "           3.7356e-01, -4.9829e-01]]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
